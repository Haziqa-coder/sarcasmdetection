{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Import Plotting Libararies\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model Evaluation Libraries\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The only thing I got from college is a caffeine addiction</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I love it when professors draw a big question mark next to my answer on an exam because I‚Äôm always like yeah I don‚Äôt either ¬Ø\\_(„ÉÑ)_/¬Ø</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember the hundred emails from companies when Covid started getting real? I‚Äôve gotten three in regards to support for protests. And only @SavageXFenty shared helpful links and actually said black lives matter... we love capitalism ü•∞üôåüèº</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Today my pop-pop told me I was not ‚Äúforced‚Äù to go to college üôÉ okay sure sureeee</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I did too, and I also reported Cancun Cruz not worrying about the heartbeats of his constituents without electricity or heat when he fled to Mexico.</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0  0            \n",
       "1  1            \n",
       "2  2            \n",
       "3  3            \n",
       "4  4            \n",
       "\n",
       "                                                                                                                                                                                                                                          tweet  \\\n",
       "0  The only thing I got from college is a caffeine addiction                                                                                                                                                                                      \n",
       "1  I love it when professors draw a big question mark next to my answer on an exam because I‚Äôm always like yeah I don‚Äôt either ¬Ø\\_(„ÉÑ)_/¬Ø                                                                                                          \n",
       "2  Remember the hundred emails from companies when Covid started getting real? I‚Äôve gotten three in regards to support for protests. And only @SavageXFenty shared helpful links and actually said black lives matter... we love capitalism ü•∞üôåüèº   \n",
       "3  Today my pop-pop told me I was not ‚Äúforced‚Äù to go to college üôÉ okay sure sureeee                                                                                                                                                               \n",
       "4  @VolphanCarol @littlewhitty @mysticalmanatee I did too, and I also reported Cancun Cruz not worrying about the heartbeats of his constituents without electricity or heat when he fled to Mexico.                                              \n",
       "\n",
       "   sarcastic  sarcasm  irony  satire  understatement  overstatement  \\\n",
       "0  1          0.0      1.0    0.0     0.0             0.0             \n",
       "1  1          1.0      0.0    0.0     0.0             0.0             \n",
       "2  1          0.0      1.0    0.0     0.0             0.0             \n",
       "3  1          1.0      0.0    0.0     0.0             0.0             \n",
       "4  1          1.0      0.0    0.0     0.0             0.0             \n",
       "\n",
       "   rhetorical_question  \n",
       "0  0.0                  \n",
       "1  0.0                  \n",
       "2  0.0                  \n",
       "3  0.0                  \n",
       "4  0.0                  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#expanding the dispay of text sms column\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "dataset = pd.read_csv(\"sarcasm_dataset.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[['tweet', 'sarcastic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3468,)\n",
      "(3468,)\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "X = data['tweet'].fillna(' ')\n",
    "Y = data[\"sarcastic\"]\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extration Using Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_bow = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = vectorizer.build_analyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '04',\n",
       " '06',\n",
       " '06z',\n",
       " '0and',\n",
       " '0c2ifipy8l',\n",
       " '0h6fxmjzf5',\n",
       " '0qwoj3teph',\n",
       " '0zga3r9d1v',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '100000',\n",
       " '100t',\n",
       " '100x',\n",
       " '101',\n",
       " '104',\n",
       " '10pm',\n",
       " '10s',\n",
       " '10th',\n",
       " '11',\n",
       " '110',\n",
       " '12',\n",
       " '121',\n",
       " '127',\n",
       " '12hrs',\n",
       " '12m',\n",
       " '13',\n",
       " '13th',\n",
       " '14',\n",
       " '143',\n",
       " '14th',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '15th',\n",
       " '16',\n",
       " '165',\n",
       " '17',\n",
       " '175',\n",
       " '18',\n",
       " '18th',\n",
       " '19',\n",
       " '1960s',\n",
       " '1966',\n",
       " '1989',\n",
       " '1995',\n",
       " '1997',\n",
       " '19th',\n",
       " '1ksb5xwxji',\n",
       " '1lkhvyupie',\n",
       " '1m',\n",
       " '1st',\n",
       " '1uputyqchg',\n",
       " '1vqoutjput',\n",
       " '1zfffxpisv',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2000s',\n",
       " '2004',\n",
       " '2007',\n",
       " '200m',\n",
       " '2011',\n",
       " '2012',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019',\n",
       " '2020',\n",
       " '2021',\n",
       " '2022',\n",
       " '2024',\n",
       " '2027',\n",
       " '206',\n",
       " '20s',\n",
       " '20th',\n",
       " '20years',\n",
       " '20yearsofblue',\n",
       " '20yr',\n",
       " '21',\n",
       " '21st',\n",
       " '22',\n",
       " '22nd',\n",
       " '23',\n",
       " '2348',\n",
       " '24',\n",
       " '240',\n",
       " '24th',\n",
       " '25',\n",
       " '256',\n",
       " '25pm',\n",
       " '26',\n",
       " '260k',\n",
       " '26th',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '2am',\n",
       " '2br',\n",
       " '2days',\n",
       " '2e2bcmrovk',\n",
       " '2hjxefatmr',\n",
       " '2k',\n",
       " '2lzu6m7yxx',\n",
       " '2nd',\n",
       " '2ogpqgh5n2',\n",
       " '2pm',\n",
       " '2swhc9dabm',\n",
       " '2wjnql5zdr',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '30am',\n",
       " '30pm',\n",
       " '30s',\n",
       " '30secs',\n",
       " '30weds21',\n",
       " '31',\n",
       " '312',\n",
       " '32',\n",
       " '324jzgselj',\n",
       " '32uhmi4wjj',\n",
       " '32watto',\n",
       " '35pm',\n",
       " '36',\n",
       " '38',\n",
       " '39',\n",
       " '3am',\n",
       " '3b7cjql2go',\n",
       " '3cnhevm2ns',\n",
       " '3eidxqpotj',\n",
       " '3g',\n",
       " '3i5dsyeqpe',\n",
       " '3in1',\n",
       " '3ivuj8ol4g',\n",
       " '3rd',\n",
       " '3rdtest',\n",
       " '3vxqnbqjqr',\n",
       " '3yearsdown',\n",
       " '40',\n",
       " '400',\n",
       " '404',\n",
       " '41',\n",
       " '420',\n",
       " '43',\n",
       " '45',\n",
       " '45th',\n",
       " '48',\n",
       " '4am',\n",
       " '4f2o2wrlyz',\n",
       " '4ipxak0jpu',\n",
       " '4onzzt8ze8',\n",
       " '4pgrthnksq',\n",
       " '4rth',\n",
       " '4th',\n",
       " '4ths',\n",
       " '50',\n",
       " '500',\n",
       " '500m',\n",
       " '50s',\n",
       " '54am',\n",
       " '55',\n",
       " '56',\n",
       " '56nnuoihxh',\n",
       " '58',\n",
       " '599xkd9ryq',\n",
       " '5am',\n",
       " '5bn',\n",
       " '5nxuz4erqq',\n",
       " '5oejqz7r0j',\n",
       " '5riwnacmm2',\n",
       " '5soz',\n",
       " '5tame4aso6',\n",
       " '5th',\n",
       " '5up',\n",
       " '60',\n",
       " '600',\n",
       " '6000',\n",
       " '60s',\n",
       " '62',\n",
       " '63',\n",
       " '65',\n",
       " '658',\n",
       " '68',\n",
       " '69',\n",
       " '6hr',\n",
       " '6kpjlezqof',\n",
       " '6kqc3gzfn5',\n",
       " '6pm',\n",
       " '6s3mfrslu1',\n",
       " '6th',\n",
       " '70',\n",
       " '700',\n",
       " '72',\n",
       " '75',\n",
       " '750',\n",
       " '750g',\n",
       " '76ers',\n",
       " '77',\n",
       " '772827',\n",
       " '7963',\n",
       " '7am',\n",
       " '7lebt3knf5',\n",
       " '80',\n",
       " '800',\n",
       " '82mdwmki3f',\n",
       " '85ykt7xngm',\n",
       " '8763',\n",
       " '8am',\n",
       " '8eqyrglbcf',\n",
       " '8ncex1fgtf',\n",
       " '8oz',\n",
       " '8pm',\n",
       " '8th',\n",
       " '8vtofksam5',\n",
       " '90',\n",
       " '900pc9kw9n',\n",
       " '91',\n",
       " '95',\n",
       " '97',\n",
       " '999',\n",
       " '9fjdbslka1',\n",
       " '9hj9zhhefi',\n",
       " '9k',\n",
       " '9pm',\n",
       " '9rnjrwfbgm',\n",
       " '9th',\n",
       " '9w1xibpl6x',\n",
       " '_aarava',\n",
       " '_celia_bedelia_',\n",
       " '_kayleighope',\n",
       " '_priya_r',\n",
       " 'a2aloimcls',\n",
       " 'a2jzjjho6r',\n",
       " 'a4xzjbrwxj',\n",
       " 'a7aazjywoq',\n",
       " 'a_genz_elder',\n",
       " 'aaah',\n",
       " 'aaronbastani',\n",
       " 'ab',\n",
       " 'abandoning',\n",
       " 'abby_starling',\n",
       " 'abhiishekkkkkk',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'abortion',\n",
       " 'abroad',\n",
       " 'abs',\n",
       " 'absentmindedly',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absorbing',\n",
       " 'abt',\n",
       " 'abuse',\n",
       " 'abusive',\n",
       " 'abysmal',\n",
       " 'abz',\n",
       " 'ac',\n",
       " 'academic',\n",
       " 'academicchatter',\n",
       " 'academics',\n",
       " 'academictwitter',\n",
       " 'academy',\n",
       " 'acapulco',\n",
       " 'acceleration',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'access',\n",
       " 'accidentally',\n",
       " 'acclaimed',\n",
       " 'accliverpool',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accordance',\n",
       " 'according',\n",
       " 'accordion',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accounts',\n",
       " 'accusations',\n",
       " 'accuser',\n",
       " 'aces',\n",
       " 'acf_uk',\n",
       " 'achieve',\n",
       " 'achievement',\n",
       " 'aching',\n",
       " 'acid',\n",
       " 'ackley',\n",
       " 'acknowledge',\n",
       " 'acknowledges',\n",
       " 'acknowledging',\n",
       " 'acne',\n",
       " 'acquired',\n",
       " 'act',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'active',\n",
       " 'activewear',\n",
       " 'activism',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'actuarial',\n",
       " 'acu',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adamlevine',\n",
       " 'adaptive',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'addilyn',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'adebayo',\n",
       " 'adenoviruses',\n",
       " 'adhd',\n",
       " 'adherence',\n",
       " 'adhering',\n",
       " 'adi_jord',\n",
       " 'administration',\n",
       " 'admit',\n",
       " 'admitted',\n",
       " 'adopted',\n",
       " 'adorable',\n",
       " 'adore',\n",
       " 'adores',\n",
       " 'adorinqdwt',\n",
       " 'adrian',\n",
       " 'ads',\n",
       " 'adult',\n",
       " 'adulthood',\n",
       " 'adultquestions',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'advantage',\n",
       " 'advent',\n",
       " 'adventure',\n",
       " 'adversely',\n",
       " 'advice',\n",
       " 'advised',\n",
       " 'advisor',\n",
       " 'advocate',\n",
       " 'aesthetic',\n",
       " 'aewallout',\n",
       " 'af',\n",
       " 'affairs',\n",
       " 'affecting',\n",
       " 'affirming',\n",
       " 'afford',\n",
       " 'affordable',\n",
       " 'afghanistan',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'africanglo',\n",
       " 'aftercares',\n",
       " 'afterlife',\n",
       " 'afternoon',\n",
       " 'aftershocks',\n",
       " 'againstarms',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agender',\n",
       " 'agent_marketing',\n",
       " 'agenttechaway',\n",
       " 'ages',\n",
       " 'aggretsuko',\n",
       " 'aging',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'agusriver13',\n",
       " 'ah',\n",
       " 'ahahahaha',\n",
       " 'ahdjfkfbckdkehdidb',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ahhh',\n",
       " 'ahhhhhhhh',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aide',\n",
       " 'aids',\n",
       " 'aimed',\n",
       " 'aimee_garcia',\n",
       " 'aiming',\n",
       " 'ain',\n",
       " 'aint',\n",
       " 'aiqe02ufe3',\n",
       " 'air',\n",
       " 'airbnb',\n",
       " 'aircon',\n",
       " 'airfrancefr',\n",
       " 'airlines',\n",
       " 'airplane',\n",
       " 'airpod',\n",
       " 'airpods',\n",
       " 'airport',\n",
       " 'airways',\n",
       " 'aisuruwrld',\n",
       " 'ajax',\n",
       " 'aka',\n",
       " 'akinfenwa',\n",
       " 'akip',\n",
       " 'akslfljsd',\n",
       " 'al',\n",
       " 'alabama',\n",
       " 'aladdin',\n",
       " 'alan',\n",
       " 'alarm',\n",
       " 'alarms',\n",
       " 'alas',\n",
       " 'alaska',\n",
       " 'album',\n",
       " 'albums',\n",
       " 'alcoholism',\n",
       " 'alec',\n",
       " 'alecs',\n",
       " 'alejandrathfc',\n",
       " 'alert',\n",
       " 'alex',\n",
       " 'alex_jantz',\n",
       " 'alex_olney',\n",
       " 'alexa',\n",
       " 'alexisnotonffir',\n",
       " 'alexs',\n",
       " 'alexshawespn',\n",
       " 'alfredo',\n",
       " 'algorand',\n",
       " 'algorandd',\n",
       " 'algorithm',\n",
       " 'alice',\n",
       " 'alien',\n",
       " 'alight',\n",
       " 'alisson',\n",
       " 'alive',\n",
       " 'allbirthdaysmatter',\n",
       " 'allegations',\n",
       " 'allegedly',\n",
       " 'allergic',\n",
       " 'allergies',\n",
       " 'allergy',\n",
       " 'alligator',\n",
       " 'alligatoraliens',\n",
       " 'allllll',\n",
       " 'alllllll',\n",
       " 'allow',\n",
       " 'allowance',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'alloys',\n",
       " 'allstarmusicals',\n",
       " 'allthethanksineed',\n",
       " 'almond',\n",
       " 'alongside',\n",
       " 'alpha',\n",
       " 'alpharetta',\n",
       " 'alright',\n",
       " 'alsoimapoorteacher',\n",
       " 'altec',\n",
       " 'alterior',\n",
       " 'alternate',\n",
       " 'altitudes',\n",
       " 'altomare',\n",
       " 'alzheimer',\n",
       " 'am9dutvfmq',\n",
       " 'amazing',\n",
       " 'amazon',\n",
       " 'amazonuk',\n",
       " 'amazonwishlists',\n",
       " 'ambassador',\n",
       " 'ambience',\n",
       " 'ambulance',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'amish',\n",
       " 'amnestyuk',\n",
       " 'amorous',\n",
       " 'amp',\n",
       " 'amplify',\n",
       " 'amsterdam',\n",
       " 'analysis',\n",
       " 'anatomy',\n",
       " 'ancestors',\n",
       " 'ancient',\n",
       " 'anderson',\n",
       " 'andre',\n",
       " 'andrea',\n",
       " 'andrew',\n",
       " 'andrews',\n",
       " 'android',\n",
       " 'andy',\n",
       " 'andy_mansell72',\n",
       " 'anesthesia',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angelis',\n",
       " 'anger',\n",
       " 'angry',\n",
       " 'animal',\n",
       " 'animalbehaviour',\n",
       " 'animals',\n",
       " 'animaniacs',\n",
       " 'animated',\n",
       " 'animation',\n",
       " 'animations',\n",
       " 'animatronic',\n",
       " 'anime',\n",
       " 'ankle',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'annan',\n",
       " 'annewheaton',\n",
       " 'annie',\n",
       " 'anniversary',\n",
       " 'annotation',\n",
       " 'annotations',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announcer',\n",
       " 'announces',\n",
       " 'announcing',\n",
       " 'announcments',\n",
       " 'annoyance',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annoyingly',\n",
       " 'annoys',\n",
       " 'annual',\n",
       " 'anon_opin',\n",
       " 'anonymous',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answers',\n",
       " 'antacid',\n",
       " 'anthem',\n",
       " 'anthony',\n",
       " 'anthonyogogo',\n",
       " 'anti',\n",
       " 'antics',\n",
       " 'antidepressants',\n",
       " 'antidote',\n",
       " 'antihistamine',\n",
       " 'antimask',\n",
       " 'antique',\n",
       " 'antisemitic',\n",
       " 'antiseptic',\n",
       " 'antitrusthearings',\n",
       " 'antivax',\n",
       " 'antivaxxers',\n",
       " 'antonoff',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anytime',\n",
       " 'anyways',\n",
       " 'aod5lrxpmb',\n",
       " 'aoxmxf8pdv',\n",
       " 'ap',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apartments',\n",
       " 'ape',\n",
       " 'apocalypse',\n",
       " 'apologise',\n",
       " 'apologising',\n",
       " 'apologize',\n",
       " 'app',\n",
       " 'appalachian',\n",
       " 'appalled',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appears',\n",
       " 'apple',\n",
       " 'applebee',\n",
       " 'appleevent',\n",
       " 'appliance',\n",
       " 'application',\n",
       " 'applied',\n",
       " 'applies',\n",
       " 'apply',\n",
       " 'applying',\n",
       " 'appointed',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciation',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'approve',\n",
       " 'apps',\n",
       " 'april',\n",
       " 'aps',\n",
       " 'apt',\n",
       " 'aquariums',\n",
       " 'aquinasgbb',\n",
       " 'aquinasnation',\n",
       " 'aquire',\n",
       " 'ar',\n",
       " 'arcade',\n",
       " 'arcgis',\n",
       " 'archer',\n",
       " 'arcing',\n",
       " 'arctic',\n",
       " 'ardpgl0m9p',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'aren',\n",
       " 'arenaswansea',\n",
       " 'arent',\n",
       " 'argh',\n",
       " 'argoblockchain',\n",
       " 'arguably',\n",
       " 'argue',\n",
       " 'argued',\n",
       " 'arguing',\n",
       " 'argument',\n",
       " 'ariana',\n",
       " 'arianagrande',\n",
       " 'ariesbabe85',\n",
       " 'arise',\n",
       " 'arlo',\n",
       " 'arm',\n",
       " 'arman',\n",
       " 'armours',\n",
       " 'armpits',\n",
       " 'arms',\n",
       " 'arnold',\n",
       " 'aroace',\n",
       " 'aroma',\n",
       " 'arounds',\n",
       " 'array',\n",
       " 'arrest',\n",
       " 'arrived',\n",
       " 'arrow',\n",
       " 'arrows',\n",
       " 'arsehole',\n",
       " 'arsenal',\n",
       " 'art',\n",
       " 'artfully',\n",
       " 'arthur',\n",
       " 'article',\n",
       " 'artisan',\n",
       " 'artistic',\n",
       " 'artists',\n",
       " 'asajj',\n",
       " 'asap',\n",
       " 'asda',\n",
       " 'asdaserviceteam',\n",
       " 'ashamed',\n",
       " 'ashton',\n",
       " 'ashwin',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'askingforafriend',\n",
       " 'asks',\n",
       " 'asl',\n",
       " 'asleep',\n",
       " 'asos',\n",
       " 'aspect',\n",
       " 'aspects',\n",
       " 'aspire',\n",
       " 'ass',\n",
       " 'assault',\n",
       " 'assaulters',\n",
       " 'assay',\n",
       " 'assert',\n",
       " 'asses',\n",
       " 'assessment',\n",
       " 'assessments',\n",
       " 'asset',\n",
       " 'asshat',\n",
       " 'asshole',\n",
       " 'assholes',\n",
       " 'assign',\n",
       " 'assigned',\n",
       " 'assignment',\n",
       " 'assignments',\n",
       " 'assistant',\n",
       " 'assisted',\n",
       " 'assistive',\n",
       " 'assists',\n",
       " 'associate',\n",
       " 'association',\n",
       " 'assume',\n",
       " 'assuming',\n",
       " 'assumptions',\n",
       " 'astonishing',\n",
       " 'astounding',\n",
       " 'astros',\n",
       " 'asvechnicock',\n",
       " 'asylum',\n",
       " 'ate',\n",
       " 'athlete',\n",
       " 'athletes',\n",
       " 'athletic',\n",
       " 'atl',\n",
       " 'atlantic',\n",
       " 'atlvsphi',\n",
       " 'atm',\n",
       " 'atmosphere',\n",
       " 'atmospheric',\n",
       " 'atp',\n",
       " 'atptour',\n",
       " 'atrocious',\n",
       " 'attachment',\n",
       " 'attack',\n",
       " 'attacking',\n",
       " 'attacks',\n",
       " 'attapoll',\n",
       " 'attempt',\n",
       " 'attempting',\n",
       " 'attenborough',\n",
       " 'attend',\n",
       " 'attention',\n",
       " 'attentive',\n",
       " 'attire',\n",
       " 'attorneys',\n",
       " 'attracted',\n",
       " 'au',\n",
       " 'aubergine',\n",
       " 'aubviouslynot',\n",
       " 'audacity',\n",
       " 'audience',\n",
       " 'audio',\n",
       " 'august',\n",
       " 'aunt',\n",
       " 'aurora',\n",
       " 'austin',\n",
       " 'australia',\n",
       " 'australiafires',\n",
       " 'australian',\n",
       " 'author',\n",
       " 'authored',\n",
       " 'authoritarian',\n",
       " 'authorities',\n",
       " 'authority',\n",
       " 'autism',\n",
       " 'autistic',\n",
       " 'auto',\n",
       " 'autobiographical',\n",
       " 'autographed',\n",
       " 'autoimmune',\n",
       " 'automated',\n",
       " 'automatic',\n",
       " 'automatically',\n",
       " 'autumn',\n",
       " 'availability',\n",
       " 'available',\n",
       " 'avarebecca919',\n",
       " 'avatarodin',\n",
       " 'average',\n",
       " 'avery',\n",
       " 'avfc',\n",
       " 'avin',\n",
       " 'avlars',\n",
       " 'avoid',\n",
       " 'avsmph',\n",
       " 'aw',\n",
       " 'awaiting',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awfully',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awning',\n",
       " 'awonderland',\n",
       " 'awwww',\n",
       " 'axa',\n",
       " 'ayeee',\n",
       " 'ayjchan',\n",
       " 'ayo',\n",
       " 'azhar_ullah',\n",
       " 'azuliblancos',\n",
       " 'b90zfws0lf',\n",
       " 'babe',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'babymilkaction',\n",
       " 'babyyy',\n",
       " 'bachelor',\n",
       " 'bachelors',\n",
       " 'backbone',\n",
       " 'backed',\n",
       " 'backers',\n",
       " 'background',\n",
       " 'backstage',\n",
       " 'backup',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'badass',\n",
       " 'baddie',\n",
       " 'badge',\n",
       " 'badly',\n",
       " 'bae',\n",
       " 'baffles',\n",
       " 'baffling',\n",
       " 'bag',\n",
       " 'bagel',\n",
       " 'bagels',\n",
       " 'bags',\n",
       " 'baileysfantasycamptn',\n",
       " 'baja',\n",
       " 'baked',\n",
       " 'baking',\n",
       " 'bakircioglu',\n",
       " 'balance',\n",
       " 'balanced',\n",
       " 'bald',\n",
       " 'bales',\n",
       " 'ball',\n",
       " 'balldontstop',\n",
       " 'balloons',\n",
       " 'balls',\n",
       " 'bam',\n",
       " 'bama',\n",
       " 'bambi',\n",
       " 'ban',\n",
       " 'banalincolnlogs',\n",
       " 'banana',\n",
       " 'bananas',\n",
       " 'bananna',\n",
       " 'band',\n",
       " 'bandana',\n",
       " 'bandcamp',\n",
       " 'bandied',\n",
       " 'bands',\n",
       " 'bandstand',\n",
       " 'bane',\n",
       " 'bang',\n",
       " 'banger',\n",
       " 'bangs',\n",
       " 'bank',\n",
       " 'banned',\n",
       " 'banning',\n",
       " 'banshee',\n",
       " 'bar',\n",
       " 'barbara',\n",
       " 'barbie',\n",
       " 'barbosabox',\n",
       " 'barcacentre',\n",
       " 'barcelona',\n",
       " 'bard',\n",
       " 'bare',\n",
       " 'barefoot',\n",
       " 'barely',\n",
       " 'bark',\n",
       " 'barnes',\n",
       " 'barney',\n",
       " 'barre',\n",
       " 'barrett',\n",
       " 'barrymore',\n",
       " 'bars',\n",
       " 'bart',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'baseline',\n",
       " 'basement',\n",
       " 'bashing',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basil',\n",
       " 'basis',\n",
       " 'basket',\n",
       " 'basketball',\n",
       " 'baskets',\n",
       " 'baskin',\n",
       " 'bass',\n",
       " 'bastard',\n",
       " 'bastila',\n",
       " 'bat',\n",
       " 'bateman',\n",
       " 'bath',\n",
       " 'bathroom',\n",
       " 'batters',\n",
       " 'battery',\n",
       " 'batting',\n",
       " 'battles',\n",
       " 'baylor',\n",
       " 'bb',\n",
       " 'bbc',\n",
       " 'bbc2',\n",
       " 'bbcamerica',\n",
       " 'bbcbodyclock',\n",
       " 'bbccricket',\n",
       " 'bbcfootball',\n",
       " 'bbcgoodfood',\n",
       " 'bbchospital',\n",
       " 'bbcolympics',\n",
       " 'bbcone',\n",
       " 'bbcpn',\n",
       " 'bbcqt',\n",
       " 'bbcr4today',\n",
       " 'bbcradio4',\n",
       " 'bbcscotborders',\n",
       " 'bbcsnooker',\n",
       " 'bbcsport',\n",
       " 'bbctms',\n",
       " 'bbhd4bwonn',\n",
       " 'bbiq7zys3x',\n",
       " 'bbmatt',\n",
       " 'bbq',\n",
       " 'bc',\n",
       " 'bcd3pnemca',\n",
       " 'bcos',\n",
       " 'bday',\n",
       " 'bdubs',\n",
       " 'beach',\n",
       " 'beaming',\n",
       " 'bean',\n",
       " 'beans',\n",
       " 'bear',\n",
       " 'bearbadger2',\n",
       " 'bears',\n",
       " 'beasley',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'beaten',\n",
       " 'beats',\n",
       " 'beautician',\n",
       " 'beautiful',\n",
       " 'beauty',\n",
       " 'becellison17',\n",
       " 'beckett',\n",
       " 'becomung',\n",
       " 'bed',\n",
       " 'bedroom',\n",
       " 'beds',\n",
       " 'bedworth',\n",
       " 'bee',\n",
       " 'beefy',\n",
       " 'beeping',\n",
       " 'beer',\n",
       " 'beers',\n",
       " 'beesmygod_',\n",
       " 'beetwitter',\n",
       " 'befriending',\n",
       " 'begging',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'begins',\n",
       " 'beh',\n",
       " 'behalf',\n",
       " 'behaviour',\n",
       " 'beige',\n",
       " 'bekind',\n",
       " 'belated',\n",
       " 'belgiangp',\n",
       " 'belieber',\n",
       " 'beliefs',\n",
       " 'believably',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'believing',\n",
       " 'bell',\n",
       " 'bells',\n",
       " 'bellulargaming',\n",
       " 'belongs',\n",
       " 'beloved',\n",
       " 'belt',\n",
       " 'ben',\n",
       " 'benbreo',\n",
       " 'bench',\n",
       " 'bene',\n",
       " 'benedict',\n",
       " 'benefit',\n",
       " 'benefits',\n",
       " 'benirvo',\n",
       " 'bennett',\n",
       " 'bennettrun',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3468, 10188)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bow.toarray()\n",
    "X_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Using TfIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(smooth_idf=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Smoothing:\n",
      "       00  000   04   06  06z  0and  0c2ifipy8l  0h6fxmjzf5  0qwoj3teph  \\\n",
      "0     0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "2     0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "4     0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "...   ...  ...  ...  ...  ...  ...   ...         ...         ...          \n",
      "3463  0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "3464  0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "3465  0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "3466  0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "3467  0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "\n",
      "      0zga3r9d1v  ...  zqxelwrbop  zrkxo3w83f  zrtl5alnot  zsl  zsoi0mbkws  \\\n",
      "0     0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "1     0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "2     0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "3     0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "4     0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "...   ...         ...  ...         ...         ...         ...  ...          \n",
      "3463  0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "3464  0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "3465  0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "3466  0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "3467  0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "\n",
      "      zulu  zulu717  zumba  zutekh  zywia0ex4u  \n",
      "0     0.0   0.0      0.0    0.0     0.0         \n",
      "1     0.0   0.0      0.0    0.0     0.0         \n",
      "2     0.0   0.0      0.0    0.0     0.0         \n",
      "3     0.0   0.0      0.0    0.0     0.0         \n",
      "4     0.0   0.0      0.0    0.0     0.0         \n",
      "...   ...   ...      ...    ...     ...         \n",
      "3463  0.0   0.0      0.0    0.0     0.0         \n",
      "3464  0.0   0.0      0.0    0.0     0.0         \n",
      "3465  0.0   0.0      0.0    0.0     0.0         \n",
      "3466  0.0   0.0      0.0    0.0     0.0         \n",
      "3467  0.0   0.0      0.0    0.0     0.0         \n",
      "\n",
      "[3468 rows x 10188 columns]\n",
      "\n",
      "\n",
      "With Smoothing:\n",
      "       00  000   04   06  06z  0and  0c2ifipy8l  0h6fxmjzf5  0qwoj3teph  \\\n",
      "0     0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "2     0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "4     0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "...   ...  ...  ...  ...  ...  ...   ...         ...         ...          \n",
      "3463  0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "3464  0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "3465  0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "3466  0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "3467  0.0  0.0  0.0  0.0  0.0  0.0   0.0         0.0         0.0          \n",
      "\n",
      "      0zga3r9d1v  ...  zqxelwrbop  zrkxo3w83f  zrtl5alnot  zsl  zsoi0mbkws  \\\n",
      "0     0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "1     0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "2     0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "3     0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "4     0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "...   ...         ...  ...         ...         ...         ...  ...          \n",
      "3463  0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "3464  0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "3465  0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "3466  0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "3467  0.0         ...  0.0         0.0         0.0         0.0  0.0          \n",
      "\n",
      "      zulu  zulu717  zumba  zutekh  zywia0ex4u  \n",
      "0     0.0   0.0      0.0    0.0     0.0         \n",
      "1     0.0   0.0      0.0    0.0     0.0         \n",
      "2     0.0   0.0      0.0    0.0     0.0         \n",
      "3     0.0   0.0      0.0    0.0     0.0         \n",
      "4     0.0   0.0      0.0    0.0     0.0         \n",
      "...   ...   ...      ...    ...     ...         \n",
      "3463  0.0   0.0      0.0    0.0     0.0         \n",
      "3464  0.0   0.0      0.0    0.0     0.0         \n",
      "3465  0.0   0.0      0.0    0.0     0.0         \n",
      "3466  0.0   0.0      0.0    0.0     0.0         \n",
      "3467  0.0   0.0      0.0    0.0     0.0         \n",
      "\n",
      "[3468 rows x 10188 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#without smooth IDF\n",
    "print(\"Without Smoothing:\")\n",
    "#define tf-idf\n",
    "tf_idf_vec = TfidfVectorizer(use_idf=True, \n",
    "                        smooth_idf=False,  \n",
    "                        ngram_range=(1,1), stop_words='english') # to use only  bigrams ngram_range=(2,2)\n",
    "#transform\n",
    "tf_idf_data = tf_idf_vec.fit_transform(X)\n",
    " \n",
    "#create dataframe\n",
    "tf_idf_dataframe=pd.DataFrame(tf_idf_data.toarray(),columns = tf_idf_vec.get_feature_names())\n",
    "print(tf_idf_dataframe)\n",
    "print(\"\\n\")\n",
    " \n",
    "#with smooth\n",
    "\n",
    "tf_idf_vec_smooth = TfidfVectorizer(use_idf=True,  \n",
    "                        smooth_idf=True,  \n",
    "                        ngram_range=(1,1),stop_words='english')\n",
    "tf_idf_data_smooth = tf_idf_vec_smooth.fit_transform(X)\n",
    "print(\"With Smoothing:\")\n",
    "tf_idf_dataframe_smooth = pd.DataFrame(tf_idf_data_smooth.toarray(),columns=tf_idf_vec_smooth.get_feature_names())\n",
    "print(tf_idf_dataframe_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Using Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>unigram/bigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>feel like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>year old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>don think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>ve got</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>just want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27190</th>\n",
       "      <td>1</td>\n",
       "      <td>000 cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27191</th>\n",
       "      <td>1</td>\n",
       "      <td>000 400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27192</th>\n",
       "      <td>1</td>\n",
       "      <td>00 tip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27193</th>\n",
       "      <td>1</td>\n",
       "      <td>00 decent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27194</th>\n",
       "      <td>1</td>\n",
       "      <td>00 24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27195 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       frequency unigram/bigram\n",
       "0      25         feel like    \n",
       "1      17         year old     \n",
       "2      15         don think    \n",
       "3      14         ve got       \n",
       "4      14         just want    \n",
       "...    ..               ...    \n",
       "27190  1          000 cases    \n",
       "27191  1          000 400      \n",
       "27192  1          00 tip       \n",
       "27193  1          00 decent    \n",
       "27194  1          00 24        \n",
       "\n",
       "[27195 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vec = CountVectorizer(stop_words='english', ngram_range=(2,2))\n",
    "# matrix of ngrams\n",
    "ngrams = c_vec.fit_transform(X)\n",
    "# count frequency of ngrams\n",
    "count_values = ngrams.toarray().sum(axis=0)\n",
    "# list of ngrams\n",
    "vocab = c_vec.vocabulary_\n",
    "X_2gram  = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
    "            ).rename(columns={0: 'frequency', 1:'unigram/bigram'})\n",
    "\n",
    "X_2gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>bigram/trigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>time office closes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>team hang 10pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>sherlock sherlock sherlock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>service team hang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>pub crawl pub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25295</th>\n",
       "      <td>1</td>\n",
       "      <td>000 cases day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25296</th>\n",
       "      <td>1</td>\n",
       "      <td>000 400 000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25297</th>\n",
       "      <td>1</td>\n",
       "      <td>00 tip app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25298</th>\n",
       "      <td>1</td>\n",
       "      <td>00 decent person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25299</th>\n",
       "      <td>1</td>\n",
       "      <td>00 24 single</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25300 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       frequency              bigram/trigram\n",
       "0      4          time office closes        \n",
       "1      4          team hang 10pm            \n",
       "2      4          sherlock sherlock sherlock\n",
       "3      4          service team hang         \n",
       "4      4          pub crawl pub             \n",
       "...   ..                    ...             \n",
       "25295  1          000 cases day             \n",
       "25296  1          000 400 000               \n",
       "25297  1          00 tip app                \n",
       "25298  1          00 decent person          \n",
       "25299  1          00 24 single              \n",
       "\n",
       "[25300 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vec = CountVectorizer(stop_words='english', ngram_range=(3,3))\n",
    "# matrix of ngrams\n",
    "ngrams = c_vec.fit_transform(X)\n",
    "# count frequency of ngrams\n",
    "count_values = ngrams.toarray().sum(axis=0)\n",
    "# list of ngrams\n",
    "vocab = c_vec.vocabulary_\n",
    "X_3gram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
    "            ).rename(columns={0: 'frequency', 1:'bigram/trigram'})\n",
    "\n",
    "X_3gram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification using TFIDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Na√Øve Bayes, Logistic Regression, Random Forest, SVM, Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Models\n",
    "from sklearn import svm  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train (2323,)\n",
      "Shape of X_test (1145,)\n",
      "Shape of Y_train (2323,)\n",
      "Shape of Y_test (1145,)\n"
     ]
    }
   ],
   "source": [
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33)\n",
    "print('Shape of X_train', X_train.shape)\n",
    "print('Shape of X_test', X_test.shape)\n",
    "print('Shape of Y_train', y_train.shape)\n",
    "print('Shape of Y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature_num = 3468\n",
    "vectorizer = TfidfVectorizer(max_features=max_feature_num)\n",
    "train_vecs = vectorizer.fit_transform(X_train)\n",
    "test_vecs = TfidfVectorizer(max_features=max_feature_num, vocabulary=vectorizer.vocabulary_).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_classifier(train_vecs, y_train, test_vecs, y_test):\n",
    "    # Training\n",
    "    GB = GaussianNB()\n",
    "    GB.fit(train_vecs, y_train)\n",
    "\n",
    "    # testing\n",
    "    test_predictionGB = GB.predict(test_vecs)\n",
    "    return classification_report(test_predictionGB, y_test) , confusion_matrix(test_predictionGB, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_classifier(train_vecs, y_train, test_vecs, y_test):\n",
    "    # Training\n",
    "    LR = LogisticRegression()\n",
    "    LR.fit(train_vecs, y_train)\n",
    "\n",
    "    # testing\n",
    "    test_predictionLR = LR.predict(test_vecs)\n",
    "    return classification_report(test_predictionLR, y_test) , confusion_matrix(test_predictionLR, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_classifier(train_vecs, y_train, test_vecs, y_test):\n",
    "    # Training\n",
    "    RF = RandomForestClassifier(n_estimators = 450, max_depth=9, random_state=43)\n",
    "    RF.fit(train_vecs, y_train)\n",
    "\n",
    "    # Testing\n",
    "    test_predictionRF = RF.predict( test_vecs )\n",
    "    return classification_report(test_predictionRF, y_test), confusion_matrix(test_predictionRF, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_classifier(train_vecs, y_train, test_vecs, y_test):\n",
    "    # Training\n",
    "    SVM = svm.LinearSVC(max_iter=100)\n",
    "    SVM.fit(train_vecs, y_train)\n",
    "\n",
    "    # Testing\n",
    "    test_predictionSVM = SVM.predict(test_vecs)\n",
    "    return classification_report(test_predictionSVM, y_test), confusion_matrix(test_predictionSVM, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perceptron_classifier(train_vecs, y_train, test_vecs, y_test):\n",
    "\n",
    "    # Training\n",
    "    Per = Perceptron(tol=1e-3, random_state=0)\n",
    "    Per.fit(train_vecs, y_train)\n",
    "\n",
    "    # Testing\n",
    "    test_predictionPer = Per.predict(test_vecs)\n",
    "    return classification_report(test_predictionPer, y_test), confusion_matrix(test_predictionPer, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of SVM CLASSIFIER on TF-IDF Vectorizer\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.77      0.82       989\n",
      "           1       0.20      0.37      0.26       156\n",
      "\n",
      "    accuracy                           0.72      1145\n",
      "   macro avg       0.55      0.57      0.54      1145\n",
      "weighted avg       0.79      0.72      0.75      1145\n",
      "\n",
      "[[763 226]\n",
      " [ 98  58]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = SVM_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of SVM CLASSIFIER on TF-IDF Vectorizer\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Regression Classifier on TF-IDF Vectorizer\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.75      0.86      1137\n",
      "           1       0.01      0.38      0.02         8\n",
      "\n",
      "    accuracy                           0.75      1145\n",
      "   macro avg       0.50      0.56      0.44      1145\n",
      "weighted avg       0.99      0.75      0.85      1145\n",
      "\n",
      "[[856 281]\n",
      " [  5   3]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = LR_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of Logistic Regression Classifier on TF-IDF Vectorizer\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of  Random Forest on TF-IDF Vectorizer\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86      1145\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.75      1145\n",
      "   macro avg       0.50      0.38      0.43      1145\n",
      "weighted avg       1.00      0.75      0.86      1145\n",
      "\n",
      "[[861 284]\n",
      " [  0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = RF_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of  Random Forest on TF-IDF Vectorizer\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Naive Bayes Classifier on TF-IDF Vectorizer\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.75      0.63       628\n",
      "           1       0.44      0.24      0.31       517\n",
      "\n",
      "    accuracy                           0.52      1145\n",
      "   macro avg       0.49      0.50      0.47      1145\n",
      "weighted avg       0.50      0.52      0.49      1145\n",
      "\n",
      "[[470 158]\n",
      " [391 126]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = NB_classifier(train_vecs.todense(), y_train, test_vecs.todense(), y_test)\n",
    "print('Results of Naive Bayes Classifier on TF-IDF Vectorizer\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Perceptron Classifier on TF-IDF Vectorizer\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.77      0.77       848\n",
      "           1       0.33      0.31      0.32       297\n",
      "\n",
      "    accuracy                           0.66      1145\n",
      "   macro avg       0.55      0.54      0.54      1145\n",
      "weighted avg       0.65      0.66      0.65      1145\n",
      "\n",
      "[[657 191]\n",
      " [204  93]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = Perceptron_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of Perceptron Classifier on TF-IDF Vectorizer\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2323, 8021) (1145, 8021)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=False,ngram_range=(1,1))\n",
    "train_vecs = vectorizer.fit_transform(X_train)\n",
    "test_vecs = vectorizer.transform(X_test)\n",
    "print (train_vecs.shape, test_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We will only consider training dataset to define the vocabulary and use the same vocabulary to \n",
    "represent the test dataset (as test data is supposed to be hidden).\n",
    "\n",
    "Thus we will fit our vectorizer on the training data and use it to transform the test data,\n",
    "\n",
    "There are 8016 unique words in vocabulary.\n",
    "\n",
    "For each review in our dataset, the Frequency of words(term-frequency) is represented through a vocabulary vector of size 8016. That‚Äôs why we have 2323 such vectors in our training-set and similarly 1145 vectors of the similar shape in our test dataset.\n",
    "\n",
    "Note: binary=False argument means that we fill the vocabulary vector with term-frequency. If binary=True, the vocabulary vector is filled by the presence of words (1 if the word is present and 0 otherwise).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of SVM CLASSIFIER on BOW\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80       916\n",
      "           1       0.27      0.34      0.30       229\n",
      "\n",
      "    accuracy                           0.69      1145\n",
      "   macro avg       0.55      0.56      0.55      1145\n",
      "weighted avg       0.71      0.69      0.70      1145\n",
      "\n",
      "[[710 206]\n",
      " [151  78]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = SVM_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of SVM CLASSIFIER on BOW\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Regression Classifier on BOW \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.77      0.83      1022\n",
      "           1       0.16      0.37      0.22       123\n",
      "\n",
      "    accuracy                           0.72      1145\n",
      "   macro avg       0.53      0.57      0.53      1145\n",
      "weighted avg       0.83      0.72      0.77      1145\n",
      "\n",
      "[[783 239]\n",
      " [ 78  45]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = LR_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of Logistic Regression Classifier on BOW \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of  Random Forest on BOW \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86      1145\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.75      1145\n",
      "   macro avg       0.50      0.38      0.43      1145\n",
      "weighted avg       1.00      0.75      0.86      1145\n",
      "\n",
      "[[861 284]\n",
      " [  0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = RF_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of  Random Forest on BOW \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Naive Bayes Classifier on BOW \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.76      0.72       772\n",
      "           1       0.35      0.27      0.30       373\n",
      "\n",
      "    accuracy                           0.60      1145\n",
      "   macro avg       0.52      0.51      0.51      1145\n",
      "weighted avg       0.58      0.60      0.58      1145\n",
      "\n",
      "[[588 184]\n",
      " [273 100]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = NB_classifier(train_vecs.todense(), y_train, test_vecs.todense(), y_test)\n",
    "print('Results of Naive Bayes Classifier on BOW \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Perceptron Classifier on BOW\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.77      0.79       918\n",
      "           1       0.25      0.32      0.28       227\n",
      "\n",
      "    accuracy                           0.68      1145\n",
      "   macro avg       0.54      0.54      0.54      1145\n",
      "weighted avg       0.71      0.68      0.69      1145\n",
      "\n",
      "[[706 212]\n",
      " [155  72]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = Perceptron_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of Perceptron Classifier on BOW\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification using Bigram/Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2323, 28181) (1145, 28181)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=False,ngram_range=(2,2))\n",
    "train_vecs = vectorizer.fit_transform(X_train)\n",
    "test_vecs = vectorizer.transform(X_test)\n",
    "print (train_vecs.shape, test_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of SVM CLASSIFIER on Bi-gram\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.76      0.85      1090\n",
      "           1       0.08      0.40      0.13        55\n",
      "\n",
      "    accuracy                           0.74      1145\n",
      "   macro avg       0.52      0.58      0.49      1145\n",
      "weighted avg       0.92      0.74      0.81      1145\n",
      "\n",
      "[[828 262]\n",
      " [ 33  22]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = SVM_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of SVM CLASSIFIER on Bi-gram\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Regression Classifier on Bi-gram\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.75      0.86      1135\n",
      "           1       0.01      0.40      0.03        10\n",
      "\n",
      "    accuracy                           0.75      1145\n",
      "   macro avg       0.50      0.58      0.44      1145\n",
      "weighted avg       0.98      0.75      0.85      1145\n",
      "\n",
      "[[855 280]\n",
      " [  6   4]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = LR_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of Logistic Regression Classifier on Bi-gram\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of  Random Forest on Bi-gram\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86      1145\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.75      1145\n",
      "   macro avg       0.50      0.38      0.43      1145\n",
      "weighted avg       1.00      0.75      0.86      1145\n",
      "\n",
      "[[861 284]\n",
      " [  0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = RF_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of  Random Forest on Bi-gram\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Naive Bayes Classifier on Bi-gram \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.75      0.72       797\n",
      "           1       0.30      0.25      0.27       348\n",
      "\n",
      "    accuracy                           0.60      1145\n",
      "   macro avg       0.50      0.50      0.50      1145\n",
      "weighted avg       0.58      0.60      0.59      1145\n",
      "\n",
      "[[599 198]\n",
      " [262  86]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = NB_classifier(train_vecs.todense(), y_train, test_vecs.todense(), y_test)\n",
    "print('Results of Naive Bayes Classifier on Bi-gram \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Perceptron Classifier on Bi-gram \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.76      0.80       959\n",
      "           1       0.20      0.30      0.24       186\n",
      "\n",
      "    accuracy                           0.69      1145\n",
      "   macro avg       0.52      0.53      0.52      1145\n",
      "weighted avg       0.74      0.69      0.71      1145\n",
      "\n",
      "[[731 228]\n",
      " [130  56]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = Perceptron_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of Perceptron Classifier on Bi-gram \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2323, 34533) (1145, 34533)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=False,ngram_range=(3,3))\n",
    "train_vecs = vectorizer.fit_transform(X_train)\n",
    "test_vecs = vectorizer.transform(X_test)\n",
    "print (train_vecs.shape, test_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of SVM CLASSIFIER on Tri-gram\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86      1139\n",
      "           1       0.01      0.50      0.02         6\n",
      "\n",
      "    accuracy                           0.75      1145\n",
      "   macro avg       0.50      0.63      0.44      1145\n",
      "weighted avg       0.99      0.75      0.85      1145\n",
      "\n",
      "[[858 281]\n",
      " [  3   3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = SVM_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of SVM CLASSIFIER on Tri-gram\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Regression Classifier on Tri-gram\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86      1143\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.75      1145\n",
      "   macro avg       0.50      0.38      0.43      1145\n",
      "weighted avg       1.00      0.75      0.86      1145\n",
      "\n",
      "[[859 284]\n",
      " [  2   0]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = LR_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of Logistic Regression Classifier on Tri-gram\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of  Random Forest on Tri-gram\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86      1145\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.75      1145\n",
      "   macro avg       0.50      0.38      0.43      1145\n",
      "weighted avg       1.00      0.75      0.86      1145\n",
      "\n",
      "[[861 284]\n",
      " [  0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = RF_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of  Random Forest on Tri-gram\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Naive Bayes Classifier on Tri-gram \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.79      0.49       383\n",
      "           1       0.72      0.27      0.39       762\n",
      "\n",
      "    accuracy                           0.44      1145\n",
      "   macro avg       0.54      0.53      0.44      1145\n",
      "weighted avg       0.60      0.44      0.42      1145\n",
      "\n",
      "[[304  79]\n",
      " [557 205]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = NB_classifier(train_vecs.todense(), y_train, test_vecs.todense(), y_test)\n",
    "print('Results of Naive Bayes Classifier on Tri-gram \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Perceptron Classifier on Tri-gram \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.76      0.84      1039\n",
      "           1       0.14      0.37      0.20       106\n",
      "\n",
      "    accuracy                           0.73      1145\n",
      "   macro avg       0.53      0.57      0.52      1145\n",
      "weighted avg       0.85      0.73      0.78      1145\n",
      "\n",
      "[[794 245]\n",
      " [ 67  39]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = Perceptron_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of Perceptron Classifier on Tri-gram \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
